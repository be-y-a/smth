{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинсон Всеволод Валерьевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "x = datasets.fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold, BaseCrossValidator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from knn.classification import KNNClassifier\n",
    "from knn.model_selection import knn_cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COUNT = 60000\n",
    "X_train, y_train = x[\"data\"][:TRAIN_COUNT], x[\"target\"][:TRAIN_COUNT]\n",
    "X_test, y_test = x[\"data\"][TRAIN_COUNT:], x[\"target\"][TRAIN_COUNT:]\n",
    "features_count = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 2.1\n",
    "\n",
    "Исследовать, какой алгоритм поиска ближайших соседей будет быстрее работать в различных ситуациях.\n",
    "Для каждого объекта тестовой выборки найти 5 его ближайших соседей в обучающей для евклидовой\n",
    "метрики. Для выборки нужно выбрать подмножество признаков, по которому будет считаться расстояние, размера 10, 20, 100 (подмножество признаков выбирается один раз для всех объектов, случайно).\n",
    "Необходимо проверить все алгоритмы поиска ближайших соседей, указанные в спецификации к заданию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alg = my_own features_count = 10 indicies = [102  20 590 387  17 329 350 683 152 592]\n",
      "algsfasfsafasfsfasf\n",
      "Fitted\n",
      "neighbours time 248.4571412080004\n",
      "alg = my_own features_count = 20 indicies = [ 76 763 130 666 776 452 298 525 366 556 153 125 774 637 579 566 749 752\n",
      "  70 135]\n",
      "algsfasfsafasfsfasf\n",
      "Fitted\n",
      "neighbours time 248.6029280799994\n",
      "alg = my_own features_count = 100 indicies = [683 251 127 596  14 656 480  13 139 711 157  24 542  31 499 111 116  70\n",
      " 304  10 316  94 773 149 464 504 305 438 781 538 760 577 734 623 567 121\n",
      " 501 521 749 779 150 580  47 100 315 110 753 710 240  63 732 597  97  25\n",
      " 217 187 661 284 667 706 548 232 412 242 626 386  93 396 228  64 527 483\n",
      " 136 156 306 564 764 372 355   8 432 222 654 702 558 699  41 309 550  80\n",
      "  79 456 503 569 610 269  83 119 533 366]\n",
      "algsfasfsafasfsfasf\n",
      "Fitted\n",
      "neighbours time 454.2199622490007\n"
     ]
    }
   ],
   "source": [
    "NEIGHBOURS_COUNT = 5\n",
    "np.random.seed(117)\n",
    "\n",
    "algorithms = ['my_own', 'brute', 'ball_tree', 'kd_tree']\n",
    "features_indicies = map(lambda x: (x, np.random.choice(features_count, size=x, replace=False)), [10, 20, 100])\n",
    "\n",
    "neighbours_time = {}\n",
    "for alg in algorithms:\n",
    "    for features_number, indicies in features_indicies:\n",
    "        print(f'alg = {alg} features_count = {features_number} indicies = {indicies}')\n",
    "        X_train_reduced = X_train[:, indicies]\n",
    "        X_test_reduced = X_test[:, indicies]\n",
    "        print('algsfasfsafasfsfasf')\n",
    "        clf = KNNClassifier(n_neighbors=NEIGHBOURS_COUNT, algorithm=alg, metric='euclidean', weights='uniform')\n",
    "        clf.fit(X_train_reduced, y_train)\n",
    "        print(\"Fitted\")\n",
    "        start = timer()\n",
    "        neighbours = clf.kneighbors(X_test_reduced)\n",
    "        end = timer()\n",
    "        calc_time = end - start\n",
    "        neighbours_time[(alg, features_number)] = calc_time\n",
    "        print(f\"neighbours time {calc_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 2.2\n",
    "\n",
    "Оценить по кросс-валидации с 3 фолдами точность (долю правильно предсказанных ответов) и время\n",
    "работы k ближайших соседей в зависимости от следующих факторов:\n",
    "(a) k от 1 до 10 (только влияние на точность).\n",
    "(b) Используется евклидова или косинусная метрика.\n",
    "Для подсчёта точности можно воспользоваться функцией accuracy_score из библиотеки scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3)\n",
    "knn_cross_val_score(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', '1', '9', '2', '1', '3', '1', '4', '3', '5', '3',\n",
       "       '6', '1', '7', '2', '8', '6', '9', '4', '0', '9', '1', '1', '2',\n",
       "       '4', '3', '2', '7', '3', '8', '6', '9', '0', '5', '6', '0', '7',\n",
       "       '6', '1', '8', '7', '9', '3', '9', '8', '5', '9', '3', '3', '0',\n",
       "       '7', '4', '9', '8', '0', '9', '4', '1', '4', '4', '6', '0', '4',\n",
       "       '5', '6', '1', '0', '0', '1', '7', '1', '6', '3', '0', '2', '1',\n",
       "       '1', '7', '9', '0', '2', '6', '7', '8', '3', '9', '0', '4', '6',\n",
       "       '7', '4', '6', '8', '0', '7', '8', '3', '1'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['target'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHtklEQVR4nO3df6jddR3H8e+59879cGusmZZauul+mKiVTcMiXDnQGkGR2qAopyDVjKDtj4HkH0KINQ0C0WBEUlLN1IxqtCxQMBOX1PZHOJ2baW1zKm1tc233nv4yCPZ9Xz333u117308/tzL7znHXZ5+wQ/fezrdbrcB8vSd6A8AHJs4IZQ4IZQ4IZQ4IdRANS7ru9r/yoUxtmloQ+dYf+7OCaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHKrwCE4+ngZy5t3dbfeUd57fL7Vpf7vLV/7OkznUjunBBKnBBKnBBKnBBKnBBKnBBKnBDKOSfHTfeyi8r9mls3tm7zBqaV116//Hfl/vu1J5d7IndOCCVOCCVOCCVOCCVOCCVOCCVOCOWckzets+SCct951axyv3/lunJfOOWkt/yZ3rD+l1eU+7zG85zAKBEnhBInhBInhBInhBInhHKU0oPO1Knl3v+u0+oX6HbbpwMH6/eeWT/69NqHTi/3XR9pf++maZovX97+6NV1s+8ur53dVz/W1TS9H5Us/tlXy33BLZvLvf63zuTOCaHECaHECaHECaHECaHECaHECaEm5TnnwFnvLvedd76t3Beesqfcf3rOg+V+uHukdXvg32eW166Ytbvch9PXdMp9qDwRHO4cc2Q++tdrWreFN28prx068p/R/jgnnDsnhBInhBInhBInhBInhBInhBInhJqU55zTf3So3J+e/9CYvv/UzpTWbaTnmCN1/QtLW7cte+pnRQ8cqp/XPOMH9T7nyedat8EDB8prJyJ3TgglTgglTgglTgglTgglTgglTgg1Kc85L53z/Ji+/t7B+hz19eKRyT8frs8S12xc0ctH+p/z1v2j3IdefqV1O/Xg30b03sMZHNNXH3/cOSGUOCGUOCGUOCGUOCGUOCGUOCHUpDznHGufXru63Gf/+ImeX3vxKc+W+9FF9e/kPXLm3HIfKL4ftLNoXnlt/6v7yv3ozr+XO//PnRNCiRNCiRNCiRNCiRNCiRNCOUoZA7ffene53/WVj/X82hfOerHc18z9bblP6fSX+7f2LmrdVsz+RXntY4fOLvdfv3JhuW99eHHrdsYf9pfXNk/WXxE4HrlzQihxQihxQihxQihxQihxQihxQqhOt9v+exqX9V1d/BLH8euKrfWZ2dfnPHOcPsnx19d0yn2oyfyRbz5c79/8/Mpy7zz+l1H8NKNr09CGY/5Q3DkhlDghlDghlDghlDghlDghlDgh1KR8nvOuRz9e7ss/UT8b+K+hqeW+48gpb/kzveHpg2eV+8MPXdbzazdN0wxzzNmM5Jhz5pK95X7beT8v98unHWndltR/5c1n128q9wcvOafch/YP87zoCeDOCaHECaHECaHECaHECaHECaHECaEm5fOcw7rkgnLuf+1AuQ9u2z6an2bCGHjnaeX+7Kr5rdsTX1xXXjuzrz4IXfibG+v9hqfKfSx5nhPGGXFCKHFCKHFCKHFCKHFCKHFCqEn5POdwugP1f7OcY/bm6K7d5T5/w9tbt1e/MFReO3OY28zZZ71c/wOB3DkhlDghlDghlDghlDghlDgh1IQ9SumbNq11e/7eBeW1g9tnlPv8x3v6SAzjxWVzWrf3DEwf0Wu/9NTp5T6v2Tmi1x8L7pwQSpwQSpwQSpwQSpwQSpwQSpwQasKec+664QOt25YPf6+89r3bV432x6Fpmv657Y+ENU3TXPeljT2/9q8Ozi73Bfe8VO5He37nsePOCaHECaHECaHECaHECaHECaHECaHG7Tln/4L2r4trmqb54eo7inXK6H4YmqZpmsNXLSn3T337kXK/ac62nt/7tm1XlvvsHc/2/NonijsnhBInhBInhBInhBInhBInhBInhBq355zd6SeV+3lTej/LXPj9f5Z74rN/o6Xv5JNbtxe+dlF57f03fqfcz50ytafP1DRNc+1z9TnmnGvrrxesv0AwkzsnhBInhBInhBInhBInhBInhBq3Rymd14+U+/NHX2/d5g20fz1g0zTNqfe9Uu4vfeN95T6wZXu5D+7b17r1LzynvPboO2aV+3B23FQfKtx+8QOt2ydnPDrMq9dHJbsHD5X70p+sad3OvXVree3Q/v3lPh65c0IocUIocUIocUIocUIocUIocUKoTrfbbR2X9V3dPoZ7Zv0H27cr7xnT9755z8XlvvNg+1fhrTztsfLapdPbz2/fjL6mU+5DTe8/8lv2vL/c/7S6/tWZA49s7vm9x7NNQxuO+UNx54RQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQE/acs29W+3OP2245v7x28+fuLPcZnfrXciYbyTnn+feuKq8997vPlfvg7j3lPlk554RxRpwQSpwQSpwQSpwQSpwQSpwQasKec8J44ZwTxhlxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQqjyKwCBE8edE0KJE0KJE0KJE0KJE0KJE0L9F8xoMzwpBDIwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "Nsample = 10\n",
    "labels = list(range(10))\n",
    "for y in labels:\n",
    "    d = X_train[622, :]\n",
    "#     for idx in range(Nsample):\n",
    "#     a = fig.add_subplot(Nsample, len(labels), idx * len(labels) + y + 1)\n",
    "    plt.imshow(d.reshape((28,28)))\n",
    "    plt.axis('off')\n",
    "    if idx == 0:\n",
    "        a.set_title('{}'.format(y))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  51., 159., 253., 159.,  50.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        48., 238., 252., 252., 252., 237.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  54., 227., 253., 252., 239., 233.,\n",
       "       252.,  57.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,  60.,\n",
       "       224., 252., 253., 252., 202.,  84., 252., 253., 122.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 163., 252., 252., 252., 253., 252., 252.,\n",
       "        96., 189., 253., 167.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51., 238.,\n",
       "       253., 253., 190., 114., 253., 228.,  47.,  79., 255., 168.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  48., 238., 252., 252., 179.,  12.,  75., 121.,\n",
       "        21.,   0.,   0., 253., 243.,  50.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38., 165., 253.,\n",
       "       233., 208.,  84.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "       165.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   7., 178., 252., 240.,  71.,  19.,  28.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 253., 252., 195.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 252., 252.,\n",
       "        63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 253.,\n",
       "       252., 195.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 198., 253., 190.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 255., 253., 196.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 246., 252.,\n",
       "       112.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       253., 252., 148.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  85., 252., 230.,  25.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   7., 135., 253., 186.,  12.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 252.,\n",
       "       223.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 131.,\n",
       "       252., 225.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  85., 252., 145.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  48., 165., 252., 173.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  86.,\n",
       "       253., 225.,   0.,   0.,   0.,   0.,   0.,   0., 114., 238., 253.,\n",
       "       162.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  85., 252., 249., 146.,  48.,  29.,\n",
       "        85., 178., 225., 253., 223., 167.,  56.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        85., 252., 252., 252., 229., 215., 252., 252., 252., 196., 130.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  28., 199., 252., 252., 253.,\n",
       "       252., 252., 233., 145.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  25., 128., 252., 253., 252., 141.,  37.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blf = BatchedKNNClassifier(n_neighbors=5, algorithm='my_own', weights='uniform')\n",
    "blf.set_batch_size(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
